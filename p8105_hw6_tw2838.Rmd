---
title: "p8105_hw6_tw2838"
author: "Tianyou Wang"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(viridis)
library(dplyr)
library(rnoaa)
library(modelr)
library(purrr)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 8,
  out.width = "90%"
)
```



## Problem 1 with Given Code - NOAA

### *Data Import*

```{r weather_df, cache = TRUE}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

To obtain a distribution for $\hat{r}^2$, we'll follow basically the same procedure we used for regression coefficients: draw bootstrap samples --> fit the a model to each --> extract the value I'm concerned with --> and summarize. Here, we'll use `modelr::bootstrap` to draw 5000 bootstrap samples and `broom::glance` to produce `r.squared` values. 

### *Distribution of $\hat{r}^2$ and its 95% CI*

```{r}
weather_r2_df = 
  weather_df %>% 
  modelr::bootstrap(n = 5000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::glance)) %>% 
  select(-strap, -models) %>% 
  unnest(results)

weather_r2_df %>% 
  ggplot(aes(x = r.squared)) + 
  geom_density()
```

In this example, the $\hat{r}^2$ value is high, and the upper bound at 1 may be a cause for the generally skewed shape of the distribution. If we wanted to construct a confidence interval for $R^2$, we could take the 2.5% and 97.5% quantiles of the estimates across bootstrap samples. However, because the shape isn't symmetric, using the mean +/- 1.96 times the standard error probably wouldn't work well.

```{r}
weather_r2_df %>% 
  summarize(
    ci_r2_lower = quantile(r.squared, 0.025), 
    ci_r2_upper = quantile(r.squared, 0.975)) %>% 
  knitr::kable(digits = 3)
```



### Distribution of $\log(\beta_0 * \beta1)$ and its 95% CI

We can produce a distribution for $\log(\beta_0 * \beta1)$ using a similar approach, with a bit more wrangling before we make our plot.

```{r}
weather_logb0b1_df = 
  weather_df %>% 
  modelr::bootstrap(n = 5000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::tidy)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  select(id = `.id`, term, estimate) %>% 
  pivot_wider(
    names_from = term, 
    values_from = estimate) %>% 
  rename(beta0 = `(Intercept)`, beta1 = tmin) %>% 
  mutate(log_b0b1 = log(beta0 * beta1))


weather_logb0b1_df %>% 
  ggplot(aes(x = log_b0b1)) + 
  geom_density()


weather_logb0b1_df %>% 
  summarize(
    ci_logb0b1_lower = quantile(log_b0b1, 0.025), 
    ci_logb0b1_upper = quantile(log_b0b1, 0.975)) %>% 
  knitr::kable(digits = 3)
```

As with $r^2$, this distribution is somewhat skewed and has some outliers. 

The point of this is not to say you should always use the bootstrap -- it's possible to establish "large sample" distributions for strange parameters / values / summaries in a lot of cases, and those are great to have. But it is helpful to know that there's a way to do inference even in tough cases. 



## Problem 2 - Homicides in US Cities

### *Data Import and Cleanning*

In the following code chunk, I did:

* Creating a `city_state` variable with combine the information form `city` and `state`, eg. Baltimore, MD
* Creating a binary variable that indicates whether the homicide is solved (0 = unsolved; 1 = solved)
* Omitting cities "Dallas, TX," "Phoenix, AZ," "Kansas City, MO," and "Tulsa, AL." Their data is not complete or correct.
* Omitting people who is not White or Black
* Changing `victim_age` into numeric variable. "NA" is assignied automatically for unknown ages

```{r}
homicide_df = 
  read_csv("data/homicide-data.csv") %>%
  janitor::clean_names() %>% 
  mutate(city_state = str_c(city, state, sep = ","),
         solve_status = ifelse(disposition == "Closed by arrest", 1, 0)) %>% 
  filter(city_state != "Dallas,TX" 
         & city_state != "Phoenix,AZ" 
         & city_state != "Kansas City,MO"
         & city_state != "Tulsa,AL") %>% 
  filter(victim_race == "White" | victim_race == "Black") %>% 
  mutate(victim_age = as.numeric(victim_age)) %>%
  select(city_state, solve_status, victim_race, victim_age, victim_sex)
```


### *Fit Logistic Regression via `glm` in Baltimore,MD*

Using the `glm` function with the `family=binomial()` specified to account for the non-Gaussian outcome distribution.

In the follwing code, I fitted a logistic regression and obtained the estimate and confidence interval of the adjusted odds ratio for solving homicides comparing male victims to female victims keeping all other variables fixed.

```{r}
baltimore_df = 
  homicide_df %>% 
  filter(city_state == "Baltimore,MD")

fit_logistic = 
  baltimore_df %>% 
  glm(solve_status ~ victim_age + victim_sex + victim_race, data = ., family = binomial()) %>% 
  broom::tidy() %>% 
  mutate(OR = exp(estimate),
         CI_95_low = exp(estimate - (1.96 * std.error)),
         CI_95_up = exp(estimate + (1.96 * std.error))) %>%
  select(term, OR, CI_95_low, CI_95_up) %>% 
  filter(term == "victim_sexMale") %>% 
  knitr::kable(digits = 3)

fit_logistic
```


### *Fit Logistic Regression via `glm` in all cities*

Now run `glm` for each of the cities in your dataset, and extract the adjusted odds ratio (and CI) for solving homicides comparing male victims to female victims. Do this within a “tidy” pipeline, making use of purrr::map, list columns, and unnest as necessary to create a dataframe with estimated ORs and CIs for each city.

```{r}
homicide_nest = nest(homicide_df, data = solve_status:victim_sex) %>% 
  mutate(
    models = map(data, ~ glm(solve_status ~ victim_age + victim_sex + victim_race, data = ., family = binomial())),
    results = map(models, broom::tidy)) %>% 
  unnest(results) %>% 
  mutate(OR = exp(estimate),
         CI_95_low = exp(estimate - (1.96 * std.error)),
         CI_95_up = exp(estimate + (1.96 * std.error))) %>%
  select(city_state, term, OR, CI_95_low, CI_95_up)

adj_OR_sex =
  homicide_nest %>% 
  filter(term == "victim_sexMale")

adj_OR_sex %>% 
  knitr::kable(digits = 3)
```


### *Plot of Estimated Adjusted ORs and CIs among Cities*

```{r fig.width=12, fig.height=6}
plot_1 = 
  adj_OR_sex %>% 
  mutate(city_state = fct_reorder(city_state, OR)) %>% 
  ggplot(aes(x = city_state, y = OR)) + 
  geom_point() + 
  geom_point(data = adj_OR_sex, mapping = (aes(x = city_state, y = CI_95_low, color = "blue"))) +
  geom_point(data = adj_OR_sex, mapping = (aes(x = city_state, y = CI_95_up, color = "red"))) +
  theme(axis.text.x = element_text(angle = 80, hjust = 1)) +
  scale_color_manual(values = c("blue", "red"),
                     labels = c("95% CI lower bound",
                                "95% CI upper bound"))

plot_1
```



## Problem 3 - Child Birth Weight

### *Data Import and Cleanning*

```{r}
bw_df = read_csv("./data/birthweight.csv") %>% 
  janitor::clean_names() %>% 
  mutate(babysex = ifelse(babysex == 1, "male", "female"),
         babysex = fct_infreq(babysex),
         frace = case_when(frace == 1 ~ "White", 
                           frace == 2 ~ "Black",
                           frace == 3 ~ "Asian",
                           frace == 4 ~ "Puerto Rican",
                           frace == 8 ~ "Other", 
                           frace == 9 ~ "Unknown"),
         frace = fct_infreq(frace),
         mrace = case_when(mrace == 1 ~ "White", 
                           mrace == 2 ~ "Black",
                           mrace == 3 ~ "Asian",
                           mrace == 4 ~ "Puerto Rican",
                           mrace == 8 ~ "Other"),
         mrace = fct_infreq(mrace),
         malform = as.character(malform))
```

Let's look at missing data in this data set.

```{r}
miss_df = 
  bw_df %>% 
  filter(bhead == 0 | blength == 0 | bwt == 0 | delwt == 0 | fincome == 0 
         | frace == "Unknown" | gaweeks == 0 | menarche == 0 | mheight == 0
         | momage == 0 | ppbmi == 0 | ppwt == 0 | wtgain == 0)
```

There are a total of `r nrow(miss_df)` observations that contain missing data.

To simplify the analysis process, I will just drop these 21 observations since the size of missing data is very small compared to the original data set.

```{r}
bw_clean_df = anti_join(bw_df, miss_df)
```



### *Fit Linear Regression Model*

#### **Explore the relationships: t-test**

First, I will fit all main effects in a linear regression model and run the default t-test to see if the association is significant via `broom::tidy()`.

```{r}
fit = lm(bwt ~ babysex + bhead + blength + delwt + fincome + frace + gaweeks + malform + menarche + mheight + momage + mrace + parity + pnumlbw + pnumsga + ppbmi + ppwt + smoken + wtgain, data = bw_clean_df)

broom::tidy(fit) %>% 
knitr::kable(digits = 3)
```

Based on t-test, at the 5% significance level, `babysex`, `bhead`, `blength`, `delwt`, `gaweeks`, `mrace(Black)`, and `smoken` have significant association with birthweight.


Since `mrace` is a categorical variable that have more than 2 levels, I will perform an ANOVA test to test them together.

```{r}
fit_null = lm(bwt ~ babysex + bhead + blength + delwt + fincome + frace + gaweeks + malform + menarche + mheight + momage + parity + pnumlbw + pnumsga + ppbmi + ppwt + smoken + wtgain, data = bw_clean_df)

fit_alt = lm(bwt ~ babysex + bhead + blength + delwt + fincome + frace + gaweeks + malform + menarche + mheight + momage + mrace + parity + pnumlbw + pnumsga + ppbmi + ppwt + smoken + wtgain, data = bw_clean_df)

anova(fit_null, fit_alt) %>% 
  pull("Pr(>F)")
```

Since the Pr(>F) = `r anova(fit_null, fit_alt) %>% pull("Pr(>F)")` < 0.05, there is evidence to show that there a significant association between `mrace` and birthweight.


#### **Visualization**

Visualize relationships between birthweight and some variables, including `babysex`, `bhead`, `blength`, `delwt`, `gaweeks`, `mrace`, and `smoken`, through correlation and boxplots.


##### *Correlation for Continuous Variable*

```{r}
library("PerformanceAnalytics")
```

Checking correlation among `bwt`, `bhead`, `blength`, `delwt`, `gaweeks`, and `smoken`.

```{r}
corr_df = 
  bw_clean_df %>% 
  select(bwt, bhead, blength, delwt, gaweeks, smoken) %>%
  chart.Correlation(histogram = TRUE)
```

According to the correlation coefficient, there is no pair of interested co-variables that are highly correlated (> 0.8). So we can include all co-variables in the final model. All `bhead` (baby’s head circumference at birth), `blength` (baby’s length at birth), `delwt` (mother’s weight at delivery), and `gaweeks` (gestational age in weeks) are positively correlated with `bwt` (baby’s birth weight). However, `smoken` (average number of cigarettes smoked per day during pregnancy), smoking negatively correlates with `bwt`.


##### *Boxplot for Categorical Variables*

1. birthweight and `babysex`

```{r}
bw_clean_df %>% 
  ggplot(aes(x = babysex, y = bwt, color = babysex)) +
  geom_boxplot()
```

Comparing birthweight among sex groups, grils have lower mean birthweight, lower 25% quartile, and lower 75% quartile than boys. Both groups have more extreme low birthweight than extreme high birthweight. However, more girls have extreme low birthweight.


2. birthweight and `mrace`

```{r}
bw_clean_df %>% 
  ggplot(aes(x = mrace, y = bwt, color = mrace)) +
  geom_boxplot()
```

Comparing among mothers' race groups, for Black mothers, their kids turned to have much lower birthweight, lower 25% quartile, and lower 75% quartile than other kids whose mothers are not Black. Black mothers also turn to give birth to kids who have extreme low birthweight. 

In conclusion, since the birth weight distributions are so different among sex groups and mothers' race groups, we will include both `babysex` and `mrace` in the linear regression model.


#### **Final proposed model**

Finally, I propose the model for birthweight as a linear regression model which include `babysex` (baby's sex), `bhead` (baby’s head circumference at birth), `blength` (baby’s length at birth), `delwt` (mother’s weight at delivery), `gaweeks` (gestational age in weeks), `mrace` (mother's race), `smoken` (average number of cigarettes smoked per day during pregnancy) as predicting variables. 

```{r}
fit_final = lm(bwt ~ babysex + bhead + blength + delwt + gaweeks + mrace + smoken, data = bw_clean_df)

broom::tidy(fit_final) %>% 
knitr::kable(digits = 3)
```



### *Plot of Model Residuals*

```{r}
plot_2 = 
bw_clean_df %>% 
  modelr::add_residuals(fit_final) %>%
  modelr::add_predictions(fit_final) %>% 
  select(pred, resid, bwt) %>% 
  ggplot(aes(x = pred, y = resid)) + 
  geom_point(alpha = .3, size = 1) + 
  labs(x = "Fitted Value", 
       y = "Model Residual")
  
plot_2
```

The model residuals kind of follw a normal distribution, which is part of the linear regression assumption. 



### *Model Comparison*

Fit each candidate model to the cleaned dataset.

```{r}
fit_my = fit_final

fit_1 = lm(bwt ~ blength + gaweeks, data = bw_clean_df)

fit_2 = lm(bwt ~ bhead + blength + babysex + bhead*blength + bhead*babysex + blength*babysex + bhead*blength*babysex, data = bw_clean_df)
```


Using `crossv_mc` to preforms the training/testing split multiple times, and stores the datasets using list columns.

```{r}
bw_clean_df2 = 
  bw_clean_df %>% 
  select(bwt, babysex, bhead, blength, delwt, gaweeks, mrace, smoken)

cv_df =
  crossv_mc(bw_clean_df2, 100) %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)) %>% 
  mutate(
    fit_my  = map(train, ~lm(bwt ~ babysex + bhead + blength + delwt + gaweeks + mrace + smoken, data = .x)),
    fit_1     = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
    fit_2  = map(train, ~lm(bwt ~ bhead + blength + babysex + bhead*blength + bhead*babysex + blength*babysex + bhead*blength*babysex, data = .x))) %>% 
  mutate(
    rmse_my = map2_dbl(fit_my, test, ~rmse(model = .x, data = .y)),
    rmse_1    = map2_dbl(fit_1, test, ~rmse(model = .x, data = .y)),
    rmse_2 = map2_dbl(fit_2, test, ~rmse(model = .x, data = .y)))
```


#### **Plot the prediction error distribution for each models**

```{r}
cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse, color = model)) + 
  geom_violin()

```

Based on these results, all the model have high RMSEs. However, my model has a reletively better prediction accuracy than the given two models.


